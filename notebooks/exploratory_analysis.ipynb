{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoAssistAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import chromadb\n",
    "import langchain\n",
    "import sqlalchemy\n",
    "import langchain_openai\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting to Know LangChain**\n",
    "\n",
    "[Oficial documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "\n",
    "LangChain is a powerful framework designed to simplify the development of applications that use language models (LLMs). Its modular structure and versatility allow developers to build a wide range of solutions, from simple automation tasks to complex systems like chatbots, question-and-answer platforms, and more.\n",
    "\n",
    "---\n",
    "\n",
    "**What is LangChain?**  \n",
    "LangChain is an open-source library that bridges the gap between LLMs and real-world applications by enabling seamless integration with various tools, data sources, and workflows. Its goal is to simplify the development process while offering robust capabilities for building intelligent applications.\n",
    "\n",
    "---\n",
    "\n",
    "**Core Features of LangChain**\n",
    "\n",
    "1. **Modularity and Customization**  \n",
    "   LangChain's modular design allows developers to integrate components like LLMs, prompt templates, memory, and agents. Each component can be customized to meet specific requirements, making the framework flexible and versatile.\n",
    "\n",
    "2. **Integration with External Data**  \n",
    "   One of LangChain's key features is Retrieval-Augmented Generation (RAG), enabling applications to retrieve and use external data sources like web content, documents, or APIs to provide more accurate and context-aware responses.\n",
    "\n",
    "3. **Memory Management**  \n",
    "   LangChain provides various types of memory, such as buffer memory and conversation memory, allowing applications to maintain context and improve user interactions over time.\n",
    "\n",
    "4. **Agent Framework**  \n",
    "   LangChain supports agents capable of dynamically deciding which tools or APIs to use based on user inputs, adding a layer of intelligence to your applications.\n",
    "\n",
    "5. **Wide Compatibility**  \n",
    "   It works seamlessly with a variety of LLMs, such as OpenAI's GPT models, Hugging Face transformers, and custom fine-tuned models, ensuring flexibility in choosing the best model for your use case.\n",
    "\n",
    "---\n",
    "\n",
    "**Applications of LangChain**\n",
    "\n",
    "- **Chatbots**: Create intelligent and context-aware conversational agents.  \n",
    "- **Question-Answer Systems**: Build systems capable of answering domain-specific questions using RAG and external data.  \n",
    "- **Automated Processes**: Develop tools for summarizing, translating, or analyzing text data.  \n",
    "- **Custom LLM Solutions**: Fine-tune language models with LangChain to address unique business problems.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Use LangChain?**\n",
    "\n",
    "LangChain simplifies the integration of language models with external tools and data sources, accelerating the development of sophisticated AI-driven applications. Whether you're building a chatbot, a data-powered assistant, or a customized LLM, LangChain offers the tools and flexibility to bring your ideas to life.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Diving Deeper into LangChain Components**\n",
    "\n",
    "LangChain’s architecture is built around several core components, each designed to perform a specific function that simplifies the integration and application of large language models (LLMs). Below, we’ll explore these components in detail:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Models**  \n",
    "The **model** is the heart of LangChain. It interacts with the language model (LLM) to generate predictions, completions, or responses.\n",
    "\n",
    "- **Supported Models:**  \n",
    "  LangChain supports a wide range of LLMs, including:\n",
    "  - OpenAI's GPT (e.g., GPT-3.5, GPT-4).\n",
    "  - Hugging Face Transformers.\n",
    "  - Open-source models (e.g., Llama, BLOOM, Falcon).\n",
    "  \n",
    "- **Customization:**  \n",
    "  Developers can fine-tune models, adjust hyperparameters, and incorporate specialized pre-trained models for domain-specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Prompts**  \n",
    "Prompts define how input is structured and presented to the LLM. Crafting effective prompts is crucial for achieving accurate and relevant responses.\n",
    "\n",
    "- **Prompt Templates:**  \n",
    "  LangChain provides tools for creating reusable templates with placeholders for dynamic inputs.  \n",
    "  Example:  \n",
    "  ```python\n",
    "  from langchain.prompts import PromptTemplate\n",
    "\n",
    "  prompt = PromptTemplate(\n",
    "      input_variables=[\"context\", \"question\"],\n",
    "      template=\"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
    "  )\n",
    "  ```\n",
    "  \n",
    "- **Prompt Optimization:**  \n",
    "  LangChain facilitates testing and iteration of prompts to maximize model performance.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Memory**  \n",
    "Memory allows the system to retain information between interactions, making applications context-aware.\n",
    "\n",
    "- **Types of Memory:**  \n",
    "  - **ConversationBufferMemory:** Stores the entire conversation history.  \n",
    "  - **ConversationSummaryMemory:** Summarizes past interactions to maintain context efficiently.  \n",
    "  - **VectorStoreRetrieverMemory:** Uses embeddings to retrieve relevant context dynamically.\n",
    "\n",
    "- **Use Case:**  \n",
    "  For chatbots, memory ensures that the bot understands and maintains context throughout a conversation.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Chains**  \n",
    "Chains are sequences of operations that transform inputs into outputs. LangChain allows developers to build complex workflows by chaining multiple components together.\n",
    "\n",
    "- **LLMChain:**  \n",
    "  The simplest type of chain, consisting of a prompt and an LLM.  \n",
    "  Example:  \n",
    "  ```python\n",
    "  from langchain.chains import LLMChain\n",
    "  from langchain.llms import OpenAI\n",
    "\n",
    "  llm = OpenAI(model=\"gpt-4\")\n",
    "  chain = LLMChain(llm=llm, prompt=prompt)\n",
    "  response = chain.run({\"context\": \"AI is transforming industries.\", \"question\": \"How is it used in healthcare?\"})\n",
    "  ```\n",
    "  \n",
    "- **Sequential Chains:**  \n",
    "  Combine multiple chains to perform more complex tasks, such as summarization followed by question-answering.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Tools and Agents**  \n",
    "Agents are decision-makers that dynamically decide which tools to use based on user input. Tools provide external capabilities, such as searching the web or accessing APIs.\n",
    "\n",
    "- **Tools:**  \n",
    "  Common tools include:\n",
    "  - **Web Search:** Retrieve real-time information.\n",
    "  - **Calculators:** Perform mathematical computations.\n",
    "  - **Databases:** Query structured or unstructured data.\n",
    "\n",
    "- **Agents:**  \n",
    "  Agents use prompts to decide which tool to invoke and how to handle responses.  \n",
    "  Example: An agent might search the web for information if a question cannot be answered using the LLM alone.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Data Connectors**  \n",
    "LangChain supports **Retrieval-Augmented Generation (RAG)** by integrating with external data sources. This makes LLMs more powerful and capable of providing accurate, context-specific answers.\n",
    "\n",
    "- **Data Sources:**  \n",
    "  - **Vector Databases:** Pinecone, Weaviate, FAISS.  \n",
    "  - **Document Loaders:** PDFs, Excel files, web scraping.  \n",
    "  - **APIs:** Integrate third-party APIs for live data retrieval.\n",
    "\n",
    "- **Embedding Models:**  \n",
    "  LangChain allows embeddings to be generated for indexing and searching data. This ensures relevant information is retrieved efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Evaluation**  \n",
    "LangChain includes tools for evaluating and debugging applications to ensure they meet performance requirements.\n",
    "\n",
    "- **Human-in-the-Loop (HITL):**  \n",
    "  Involve human evaluators to assess the quality of responses.  \n",
    "- **Automated Evaluation:**  \n",
    "  Use metrics like BLEU, ROUGE, or accuracy to measure performance.\n",
    "\n",
    "---\n",
    "\n",
    "**8. Deployment**  \n",
    "LangChain applications can be deployed on various platforms, making them scalable and production-ready.\n",
    "\n",
    "- **Cloud Platforms:** AWS, GCP, Azure.  \n",
    "- **Dockerization:** Containerize LangChain apps for easy deployment.  \n",
    "- **Integration with APIs:** Expose the functionality as RESTful APIs for external use.\n",
    "\n",
    "---\n",
    "\n",
    "**9. Advanced Features**  \n",
    "- **Streaming:** LangChain supports streaming responses for real-time applications like live chat interfaces.  \n",
    "- **Callbacks:** Monitor and log the internal workflow of chains and agents for debugging or tracking.\n",
    "\n",
    "---\n",
    "\n",
    "**Why These Components Matter**  \n",
    "Each component is modular and can be independently configured, allowing developers to:\n",
    "- Customize solutions for specific use cases.\n",
    "- Scale applications without overhauling existing structures.\n",
    "- Ensure high performance and efficiency by leveraging the best tools and integrations.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to focus on a specific component, or provide an example project that ties these components together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Defining the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the API key\n",
    "with open('../ignore/secret_key.json') as f:\n",
    "    os.environ['OPENAI_API_KEY'] = json.load(f)['secret_key']\n",
    "    \n",
    "\n",
    "# Defines the LLM\n",
    "# Creates an instance of a Large Language Model (LLM), specifically one provided by OpenAI\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature is a hyperparameter that influences the randomness of the responses generated by the model. A higher temperature value (usually ranging from 0 to 1) promotes more creative and varied responses. On the other hand, a lower temperature tends to cause the model to produce more deterministic and possibly more predictable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the prompt to LLM and capture the response\n",
    "nome = llm.invoke(\"I want to open a Japanese food restaurant. Suggest a fancy name for it.\")\n",
    "print(nome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, the string “I want to open a Japanese food restaurant. Suggest a fancy name for it.” serves as the prompt or input to the language model. It describes the task the user wants the model to perform: creatively generating a name for a new Japanese food restaurant. The model will use its natural language training and prior knowledge to generate a response that meets this request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates in the context of LangChain refer to structured ways of formatting input to large language models (LLMs) to improve their performance and adherence to desired behaviors.\n",
    "\n",
    "A prompt template defines a template sequence with placeholder variables that can be populated dynamically. This allows you to construct prompts in a consistent and programmatic manner, rather than hard-coding full prompts.\n",
    "\n",
    "Prompt templates in LangChain provide a structured and extensible way to interface with LLMs, making it easy to explore and optimize prompting strategies to improve language model performance on specific tasks or domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a {cuisine} restaurant. Suggest a fancy name for it.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code defines a PromptTemplate, a framework that allows you to create dynamic prompts for use with Large Language Models (LLMs). This approach is particularly useful when you want to generate custom prompts based on specific variables or when you want to reuse a prompt format with different data sets.\n",
    "\n",
    "**input_variables = ['cuisine']**: Defines a list of variables that can be used to populate the template. In this case, there is a single variable called 'cuisine'. This variable acts as a placeholder that will be replaced with a specific value when the template is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previously defined template to generate a specific prompt,\n",
    "# inserting the value \"Italiana\" in place of the variable culinary\n",
    "p = prompt_template_name.format(cuisine = \"Mexican\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Sequences with LLMChain\n",
    "\n",
    "Chains in LangChain are sequences of operations that can process inputs and generate outputs by combining multiple components, including large language models (LLMs), other chains, and specialized tools or utilities.\n",
    "\n",
    "An LLMChain is a type of chain that allows you to interact with a large language model (LLM) in a structured way. It provides a simple interface for passing inputs to the LLM and retrieving its outputs.\n",
    "\n",
    "The LLMChain serves as a building block for many other constructs in LangChain, such as agents, tools, and more advanced chain types. By encapsulating the LLM interaction logic in a reusable and extensible component, LLMChain simplifies the process of building applications that leverage large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain and activate verbose\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, verbose = True)\n",
    "\n",
    "# Invoke the chain by passing a parameter to the prompt\n",
    "chain.invoke(\"Brazilian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code creates an instance of LLMChain, a class designed to chain or sequence operations using an LLM. This instance is configured to use a specific language model and a predefined prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain and activate verbose\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, verbose = True)\n",
    "\n",
    "# Invoke the chain by passing a parameter to the prompt\n",
    "chain.invoke(\"Thai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sequential Chain\n",
    "\n",
    "A SimpleSequentialChain in LangChain is a chain type that executes a sequence of components (e.g. LLMs, tools, other chains) in a predefined order. It is one of the most basic and commonly used chain types in LangChain.\n",
    "\n",
    "A sample use case for SimpleSequentialChain could be a question answering system where:\n",
    "\n",
    "- The first component is an LLM that analyzes the input question.\n",
    "- The second component is a tool that retrieves relevant documents from a database.\n",
    "- The third component is another LLM that generates an answer based on the question and the retrieved documents.\n",
    "\n",
    "By chaining these components together into a SimpleSequentialChain, you can create a more complex and capable system while maintaining a modular and extensible architecture.\n",
    "\n",
    "While SimpleSequentialChain is useful for linear workflows, LangChain also provides other chain types such as ConditionalChain and SequentialChain for more complex control flows and branching logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets LLM with lower temperature\n",
    "llm = OpenAI(temperature = 0.6)\n",
    "\n",
    "# Create the prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a {cuisine} restaurant. Suggest a fancy name for it.\")\n",
    "\n",
    "\n",
    "# Create the chain\n",
    "chain_1 = LLMChain(llm = llm, prompt = prompt_template_name)\n",
    "\n",
    "\n",
    "# Create another prompt template\n",
    "prompt_template_items = PromptTemplate( input_variables = ['restaurant name'], template = \"\"\"Suggest some menu items for {restaurant_name}\"\"\")\n",
    "\n",
    "\n",
    "# Create the chain\n",
    "chain_2 = LLMChain(llm = llm, prompt = prompt_template_items, verbose=True)\n",
    "\n",
    "\n",
    "# Concatenates the two chains\n",
    "chain_final = SimpleSequentialChain(chains = [chain_1, chain_2])\n",
    "\n",
    "\n",
    "# Invoca a chain\n",
    "print(chain_final.invoke(\"Indiana\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chain\n",
    "\n",
    "SequentialChain is a more advanced version of SimpleSequentialChain. While SimpleSequentialChain executes a fixed sequence of components, SequentialChain allows dynamic and conditional execution of components based on the outputs of previous components.\n",
    "\n",
    "A sample use case for SequentialChain could be a conversational agent that:\n",
    "\n",
    "- Uses an LLM to understand user input and determine the appropriate action.\n",
    "\n",
    "- Conditionally executes different components (e.g., database lookup, API call, calculation) based on the output of the LLM.\n",
    "\n",
    "- Optionally prompts the user for additional information if needed.\n",
    "\n",
    "- Generates a final response using another LLM, based on the outputs of previous components.\n",
    "\n",
    "By leveraging SequentialChain, you can build more intelligent and adaptive applications that can dynamically adjust their behavior based on intermediate results and states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM\n",
    "llm = OpenAI(temperature = 0.7)\n",
    "\n",
    "\n",
    "# Creating the first chain\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a {cuisine} restaurant. Suggest a fancy name for it.\")\n",
    "\n",
    "# Define the chain with an output parameter\n",
    "chain_1 = LLMChain(llm = llm, prompt = prompt_template_name, output_key = \"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the second chain\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "# Define the chain with an output parameter\n",
    "chain_2 = LLMChain(llm = llm, prompt = prompt_template_items, output_key = \"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sequence of chains\n",
    "chain = SequentialChain(chains = [chain_1, chain_2],\n",
    "                        input_variables = ['cuisine'],\n",
    "                        output_variables = ['restaurant_name', \"menu_items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"cuisine\": \"Italian\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoking the method and capturing the response\n",
    "response = chain.invoke({\"cuisine\": \"Italian\"})\n",
    "\n",
    "# Preparing the formatted string\n",
    "formatted_output = f\"Cuisine: {response['cuisine']}\\nRestaurant Name: {response['restaurant_name'].strip()}\\nMenu Items:\"\n",
    "\n",
    "# Adding each menu item to the formatted string\n",
    "menu_items = response['menu_items'].strip().split('\\n')\n",
    "for item in menu_items:\n",
    "    formatted_output += f\"\\n{item}\"\n",
    "\n",
    "# Displaying the formatted output\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Memory for LLM\n",
    "\n",
    "In LangChain, “Memory” refers to components that allow chains, agents, and other constructs to store and retrieve information about previous inputs, outputs, and intermediate states. This allows them to maintain context and make use of relevant information from the history of previous conversations or computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the chain\n",
    "name = chain.invoke(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the chain\n",
    "name = chain.invoke(\"Argentina\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the memory object\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, memory = memory)\n",
    "\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = chain.run(\"Argentina\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Chain\n",
    "\n",
    "A ConversationChain is a specialized type of chain designed to handle multi-turn conversations or dialogs with an LLM.\n",
    "\n",
    "A ConversationChain is particularly useful for building conversational agents, chatbots, or any application that requires maintaining context across multiple turns of interaction with a user. By abstracting away the complexities of managing conversation history and formatting prompts, a ConversationChain simplifies the process of building multi-turn dialog systems with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the conversation object\n",
    "conv = ConversationChain(llm = OpenAI(temperature = 0.7))\n",
    "\n",
    "print(conv.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.invoke(\"Which country has won the Football World Cup the most times?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.invoke(\"What is 30 + 12?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.invoke(\"Who is the greatest scorer in the history of the Football World Cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.invoke(\"What was the first question I asked?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conversation Buffer Window Memory\n",
    "\n",
    "ConversationBufferWindowMemory is a type of LangChain memory component designed specifically for use with ConversationChain. It provides a way to store and retrieve conversation history while limiting the amount of context retained based on a specified window size.\n",
    "\n",
    "The main advantage of ConversationBufferWindowMemory is its ability to limit the amount of context provided to the LLM, which can be important for performance and to prevent the model from becoming overwhelmed with too much irrelevant information. By adjusting the window size, you can control the tradeoff between providing enough context and avoiding excessive computational overhead.\n",
    "\n",
    "This type of memory is particularly useful for building conversational agents, chatbots, or any application that requires maintaining a continuous window of relevant recent conversation history for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the memory window\n",
    "memory = ConversationBufferWindowMemory(k = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversation chain\n",
    "conv = ConversationChain(llm = OpenAI(temperature = 0.7), memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke LLM\n",
    "conv.run(\"Who won the first Football World Cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke LLM\n",
    "conv.invoke(\"What is 10 + 19?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke LLM\n",
    "conv.invoke(\"Who was the captain of the winning team of the first Football World Cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain and VectorDB for Web Scraping\n",
    "\n",
    "ChromaDB is a vector storage database library that integrates with LangChain. It provides functionality for efficiently storing, retrieving, and searching large amounts of text data using vector embeddings and semantic similarity.\n",
    "\n",
    "https://www.trychroma.com/\n",
    "\n",
    "https://pypi.org/project/chromadb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web data extraction\n",
    "data = WebBaseLoader(\n",
    "    \"https://blog.dsacademy.com.br/como-rag-retrieval-augmented-generation-funciona-para-personalizar-os-llms/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Always check a website's robots.txt before scraping data. Don't scrape if it's not allowed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "documents = data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first document (in this case there is only one document)\n",
    "document = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary keys\n",
    "document.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 100 characters\n",
    "document.page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "document.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print formatted result\n",
    "def print_response(response: str):\n",
    "    print(\"\\n\".join(textwrap.wrap(response, width = 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index using VectorstoreIndexCreator\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    embedding=OpenAIEmbeddings(),  # Define the embeddings\n",
    ")\n",
    "vectorstore = index_creator.from_loaders([data])  # Create the index from the loader\n",
    "\n",
    "# Define the LLM for querying\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.vectorstore.as_retriever(),  # Use the retriever from the vectorstore\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# Query the index\n",
    "query = \"You are a Senior AI Engineer. Explain how RAG works in building intelligent applications.\"\n",
    "response = retrieval_chain({\"query\": query})\n",
    "\n",
    "# Print the result and sources\n",
    "print(\"Answer:\", response[\"result\"])\n",
    "print(\"\\nSource Documents:\")\n",
    "\n",
    "print_response(response=response['result'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work with VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template\n",
    "template = \"\"\"\n",
    "You are a Senior AI Engineer. {context}\n",
    "\n",
    "Please answer considering the most modern techniques you know.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "prompt = PromptTemplate(template = template, input_variables = [\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the prompt to view the format\n",
    "print(prompt.format(\n",
    "    context = \"AI Application for Customer Service Systems.\",\n",
    "    question = \"How to create a web application with LLM?\",)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings object\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code refers to initializing an instance of the OpenAIEmbeddings class, which is an interface for generating embeddings (vector representations) using models provided by OpenAI, such as the GPT language models. Embeddings are transformations of raw data, such as text, into vectors of fixed numbers, capturing semantic and contextual aspects of the original content in a way that can be processed by machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates VectorDB by converting text documents into numeric representations (embeddings)\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code is for creating a vector database using Chroma. This operation involves preparing a data structure optimized for searches and analysis based on documents and their respective embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(db)\n",
    "\n",
    "# Arguments\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "# Chain de RetrievalQA\n",
    "chain = RetrievalQA.from_chain_type(llm = ChatOpenAI(temperature = 0),\n",
    "                                    chain_type = \"stuff\",\n",
    "                                    retriever = db.as_retriever(search_kwargs = {\"k\": 1}),\n",
    "                                    chain_type_kwargs = chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code is creating an instance called chain, using the RetrievalQA class to configure a process chain focused on performing Question Answering (QA) tasks based on information retrieval. The from_chain_type method is used to specify the type of process chain and configure its main components, such as the language model and the retrieval engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"Explain what RAG is in 5 sentences\"\n",
    "\n",
    "# Response\n",
    "response = chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building Sales Expert Chatbot with LangChain and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining LLM\n",
    "gpt = ChatOpenAI(temperature = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template\n",
    "template = \"\"\"This is a conversation between a customer and a sports car sales specialist.\n",
    "\n",
    "You are the car specialist, you know sports cars well and you should always answer\n",
    "as accurately as possible.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "CarSpecialist:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt template\n",
    "prompt = PromptTemplate(input_variables = [\"history\", \"input\"], template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversation chain\n",
    "conversation = ConversationChain(prompt = prompt,\n",
    "                                 llm = gpt,\n",
    "                                 verbose = False,\n",
    "                                 memory = ConversationBufferMemory(ai_prefix = \"CarSpecialist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expert: If you're looking to sell your car, there are a few options you can consider. You can sell\n",
      "it privately, trade it in at a dealership, or use an online car selling service. Each option has its\n",
      "own pros and cons, so it's important to do some research and decide which method works best for you.\n",
      "If you have a sports car you're looking to sell, I can also help you with that process. Let me know\n",
      "if you have any specific questions or need assistance with selling your car.\n",
      "\n",
      "\n",
      "Expert: Great choice! The Opala is a classic sports car with a lot of potential buyers. To sell your\n",
      "Opala, you can start by taking some high-quality photos of the car and creating a detailed listing\n",
      "with all the relevant information such as the year, mileage, condition, and any upgrades or\n",
      "modifications. You can then post your listing on online car selling platforms, social media, or even\n",
      "local classified ads. If you need any help with pricing or marketing your Opala, feel free to ask\n",
      "for assistance.\n",
      "\n",
      "\n",
      "Expert: I recommend starting by thoroughly cleaning and detailing your Opala to make it look its\n",
      "best for potential buyers. Take high-quality photos from different angles and make sure to highlight\n",
      "any unique features or upgrades. Research the market value of similar Opalas to determine a fair\n",
      "asking price. Create a detailed listing with all the relevant information and post it on multiple\n",
      "platforms to reach a wider audience. Be prepared to negotiate with potential buyers and be\n",
      "transparent about the condition of the car. Good luck with selling your Opala!\n",
      "\n",
      "\n",
      "Expert: To transfer the car's documents to the new owner, you will need to fill out a bill of sale\n",
      "that includes the buyer's information, the sale price, and the vehicle's details. You will also need\n",
      "to sign over the title of the car to the new owner. Make sure to remove your license plates and\n",
      "cancel your insurance on the vehicle. It's also a good idea to notify your local DMV or equivalent\n",
      "authority of the sale to update their records. If you need any assistance with the paperwork or have\n",
      "any specific questions about transferring the car's documents, feel free to ask for help.\n",
      "\n",
      "\n",
      "Expert: Hiring a lawyer is not necessary for transferring the documents of your car to the new\n",
      "owner. As long as you follow the proper procedures and fill out the necessary paperwork correctly,\n",
      "you should be able to transfer ownership smoothly. However, if you have any legal concerns or\n",
      "questions about the process, it's always a good idea to consult with a lawyer for guidance. They can\n",
      "provide you with legal advice and ensure that the transfer of ownership is done correctly.\n",
      "\n",
      "\n",
      "Thanks for Using the AI-Based Customer Service System!\n"
     ]
    }
   ],
   "source": [
    "# Conversation loop limited to 5 interactions (increase the number of interactions or remove the if block)\n",
    "\n",
    "# Initialize the counter\n",
    "counter = 0\n",
    "\n",
    "# Loop\n",
    "while True:\n",
    "\n",
    "    prompt = input(prompt = \"Customer: \")\n",
    "    print()\n",
    "    result = conversation(prompt)\n",
    "    print_response(\"Expert: \" + result[\"response\"])\n",
    "    print()\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    if counter >= 5:\n",
    "        print('\\nThanks for Using the AI-Based Customer Service System!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
