{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoAssistAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import chromadb\n",
    "import langchain\n",
    "import sqlalchemy\n",
    "import langchain_openai\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting to Know LangChain**\n",
    "\n",
    "[Oficial documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "\n",
    "LangChain is a powerful framework designed to simplify the development of applications that use language models (LLMs). Its modular structure and versatility allow developers to build a wide range of solutions, from simple automation tasks to complex systems like chatbots, question-and-answer platforms, and more.\n",
    "\n",
    "---\n",
    "\n",
    "**What is LangChain?**  \n",
    "LangChain is an open-source library that bridges the gap between LLMs and real-world applications by enabling seamless integration with various tools, data sources, and workflows. Its goal is to simplify the development process while offering robust capabilities for building intelligent applications.\n",
    "\n",
    "---\n",
    "\n",
    "**Core Features of LangChain**\n",
    "\n",
    "1. **Modularity and Customization**  \n",
    "   LangChain's modular design allows developers to integrate components like LLMs, prompt templates, memory, and agents. Each component can be customized to meet specific requirements, making the framework flexible and versatile.\n",
    "\n",
    "2. **Integration with External Data**  \n",
    "   One of LangChain's key features is Retrieval-Augmented Generation (RAG), enabling applications to retrieve and use external data sources like web content, documents, or APIs to provide more accurate and context-aware responses.\n",
    "\n",
    "3. **Memory Management**  \n",
    "   LangChain provides various types of memory, such as buffer memory and conversation memory, allowing applications to maintain context and improve user interactions over time.\n",
    "\n",
    "4. **Agent Framework**  \n",
    "   LangChain supports agents capable of dynamically deciding which tools or APIs to use based on user inputs, adding a layer of intelligence to your applications.\n",
    "\n",
    "5. **Wide Compatibility**  \n",
    "   It works seamlessly with a variety of LLMs, such as OpenAI's GPT models, Hugging Face transformers, and custom fine-tuned models, ensuring flexibility in choosing the best model for your use case.\n",
    "\n",
    "---\n",
    "\n",
    "**Applications of LangChain**\n",
    "\n",
    "- **Chatbots**: Create intelligent and context-aware conversational agents.  \n",
    "- **Question-Answer Systems**: Build systems capable of answering domain-specific questions using RAG and external data.  \n",
    "- **Automated Processes**: Develop tools for summarizing, translating, or analyzing text data.  \n",
    "- **Custom LLM Solutions**: Fine-tune language models with LangChain to address unique business problems.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Use LangChain?**\n",
    "\n",
    "LangChain simplifies the integration of language models with external tools and data sources, accelerating the development of sophisticated AI-driven applications. Whether you're building a chatbot, a data-powered assistant, or a customized LLM, LangChain offers the tools and flexibility to bring your ideas to life.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Diving Deeper into LangChain Components**\n",
    "\n",
    "LangChain’s architecture is built around several core components, each designed to perform a specific function that simplifies the integration and application of large language models (LLMs). Below, we’ll explore these components in detail:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Models**  \n",
    "The **model** is the heart of LangChain. It interacts with the language model (LLM) to generate predictions, completions, or responses.\n",
    "\n",
    "- **Supported Models:**  \n",
    "  LangChain supports a wide range of LLMs, including:\n",
    "  - OpenAI's GPT (e.g., GPT-3.5, GPT-4).\n",
    "  - Hugging Face Transformers.\n",
    "  - Open-source models (e.g., Llama, BLOOM, Falcon).\n",
    "  \n",
    "- **Customization:**  \n",
    "  Developers can fine-tune models, adjust hyperparameters, and incorporate specialized pre-trained models for domain-specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Prompts**  \n",
    "Prompts define how input is structured and presented to the LLM. Crafting effective prompts is crucial for achieving accurate and relevant responses.\n",
    "\n",
    "- **Prompt Templates:**  \n",
    "  LangChain provides tools for creating reusable templates with placeholders for dynamic inputs.  \n",
    "  Example:  \n",
    "  ```python\n",
    "  from langchain.prompts import PromptTemplate\n",
    "\n",
    "  prompt = PromptTemplate(\n",
    "      input_variables=[\"context\", \"question\"],\n",
    "      template=\"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
    "  )\n",
    "  ```\n",
    "  \n",
    "- **Prompt Optimization:**  \n",
    "  LangChain facilitates testing and iteration of prompts to maximize model performance.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Memory**  \n",
    "Memory allows the system to retain information between interactions, making applications context-aware.\n",
    "\n",
    "- **Types of Memory:**  \n",
    "  - **ConversationBufferMemory:** Stores the entire conversation history.  \n",
    "  - **ConversationSummaryMemory:** Summarizes past interactions to maintain context efficiently.  \n",
    "  - **VectorStoreRetrieverMemory:** Uses embeddings to retrieve relevant context dynamically.\n",
    "\n",
    "- **Use Case:**  \n",
    "  For chatbots, memory ensures that the bot understands and maintains context throughout a conversation.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Chains**  \n",
    "Chains are sequences of operations that transform inputs into outputs. LangChain allows developers to build complex workflows by chaining multiple components together.\n",
    "\n",
    "- **LLMChain:**  \n",
    "  The simplest type of chain, consisting of a prompt and an LLM.  \n",
    "  Example:  \n",
    "  ```python\n",
    "  from langchain.chains import LLMChain\n",
    "  from langchain.llms import OpenAI\n",
    "\n",
    "  llm = OpenAI(model=\"gpt-4\")\n",
    "  chain = LLMChain(llm=llm, prompt=prompt)\n",
    "  response = chain.run({\"context\": \"AI is transforming industries.\", \"question\": \"How is it used in healthcare?\"})\n",
    "  ```\n",
    "  \n",
    "- **Sequential Chains:**  \n",
    "  Combine multiple chains to perform more complex tasks, such as summarization followed by question-answering.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Tools and Agents**  \n",
    "Agents are decision-makers that dynamically decide which tools to use based on user input. Tools provide external capabilities, such as searching the web or accessing APIs.\n",
    "\n",
    "- **Tools:**  \n",
    "  Common tools include:\n",
    "  - **Web Search:** Retrieve real-time information.\n",
    "  - **Calculators:** Perform mathematical computations.\n",
    "  - **Databases:** Query structured or unstructured data.\n",
    "\n",
    "- **Agents:**  \n",
    "  Agents use prompts to decide which tool to invoke and how to handle responses.  \n",
    "  Example: An agent might search the web for information if a question cannot be answered using the LLM alone.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Data Connectors**  \n",
    "LangChain supports **Retrieval-Augmented Generation (RAG)** by integrating with external data sources. This makes LLMs more powerful and capable of providing accurate, context-specific answers.\n",
    "\n",
    "- **Data Sources:**  \n",
    "  - **Vector Databases:** Pinecone, Weaviate, FAISS.  \n",
    "  - **Document Loaders:** PDFs, Excel files, web scraping.  \n",
    "  - **APIs:** Integrate third-party APIs for live data retrieval.\n",
    "\n",
    "- **Embedding Models:**  \n",
    "  LangChain allows embeddings to be generated for indexing and searching data. This ensures relevant information is retrieved efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Evaluation**  \n",
    "LangChain includes tools for evaluating and debugging applications to ensure they meet performance requirements.\n",
    "\n",
    "- **Human-in-the-Loop (HITL):**  \n",
    "  Involve human evaluators to assess the quality of responses.  \n",
    "- **Automated Evaluation:**  \n",
    "  Use metrics like BLEU, ROUGE, or accuracy to measure performance.\n",
    "\n",
    "---\n",
    "\n",
    "**8. Deployment**  \n",
    "LangChain applications can be deployed on various platforms, making them scalable and production-ready.\n",
    "\n",
    "- **Cloud Platforms:** AWS, GCP, Azure.  \n",
    "- **Dockerization:** Containerize LangChain apps for easy deployment.  \n",
    "- **Integration with APIs:** Expose the functionality as RESTful APIs for external use.\n",
    "\n",
    "---\n",
    "\n",
    "**9. Advanced Features**  \n",
    "- **Streaming:** LangChain supports streaming responses for real-time applications like live chat interfaces.  \n",
    "- **Callbacks:** Monitor and log the internal workflow of chains and agents for debugging or tracking.\n",
    "\n",
    "---\n",
    "\n",
    "**Why These Components Matter**  \n",
    "Each component is modular and can be independently configured, allowing developers to:\n",
    "- Customize solutions for specific use cases.\n",
    "- Scale applications without overhauling existing structures.\n",
    "- Ensure high performance and efficiency by leveraging the best tools and integrations.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to focus on a specific component, or provide an example project that ties these components together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Defining the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the API key\n",
    "with open('../ignore/secret_key.json') as f:\n",
    "    os.environ['OPENAI_API_KEY'] = json.load(f)['secret_key']\n",
    "    \n",
    "\n",
    "# Defines the LLM\n",
    "# Creates an instance of a Large Language Model (LLM), specifically one provided by OpenAI\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature is a hyperparameter that influences the randomness of the responses generated by the model. A higher temperature value (usually ranging from 0 to 1) promotes more creative and varied responses. On the other hand, a lower temperature tends to cause the model to produce more deterministic and possibly more predictable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Sakura Delights\"\n"
     ]
    }
   ],
   "source": [
    "# Send the prompt to LLM and capture the response\n",
    "nome = llm.invoke(\"I want to open a Japanese food restaurant. Suggest a fancy name for it.\")\n",
    "print(nome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, the string “I want to open a Japanese food restaurant. Suggest a fancy name for it.” serves as the prompt or input to the language model. It describes the task the user wants the model to perform: creatively generating a name for a new Japanese food restaurant. The model will use its natural language training and prior knowledge to generate a response that meets this request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates in the context of LangChain refer to structured ways of formatting input to large language models (LLMs) to improve their performance and adherence to desired behaviors.\n",
    "\n",
    "A prompt template defines a template sequence with placeholder variables that can be populated dynamically. This allows you to construct prompts in a consistent and programmatic manner, rather than hard-coding full prompts.\n",
    "\n",
    "Prompt templates in LangChain provide a structured and extensible way to interface with LLMs, making it easy to explore and optimize prompting strategies to improve language model performance on specific tasks or domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a {cuisine} restaurant. Suggest a fancy name for it.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code defines a PromptTemplate, a framework that allows you to create dynamic prompts for use with Large Language Models (LLMs). This approach is particularly useful when you want to generate custom prompts based on specific variables or when you want to reuse a prompt format with different data sets.\n",
    "\n",
    "**input_variables = ['cuisine']**: Defines a list of variables that can be used to populate the template. In this case, there is a single variable called 'cuisine'. This variable acts as a placeholder that will be replaced with a specific value when the template is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a Mexican restaurant. Suggest a fancy name for it.\n"
     ]
    }
   ],
   "source": [
    "# Use the previously defined template to generate a specific prompt,\n",
    "# inserting the value \"Italiana\" in place of the variable culinary\n",
    "p = prompt_template_name.format(cuisine = \"Mexican\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Sequences with LLMChain\n",
    "\n",
    "Chains in LangChain are sequences of operations that can process inputs and generate outputs by combining multiple components, including large language models (LLMs), other chains, and specialized tools or utilities.\n",
    "\n",
    "An LLMChain is a type of chain that allows you to interact with a large language model (LLM) in a structured way. It provides a simple interface for passing inputs to the LLM and retrieving its outputs.\n",
    "\n",
    "The LLMChain serves as a building block for many other constructs in LangChain, such as agents, tools, and more advanced chain types. By encapsulating the LLM interaction logic in a reusable and extensible component, LLMChain simplifies the process of building applications that leverage large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a Brazilian restaurant. Suggest a fancy name for it.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Brazilian', 'text': '\\n\"Sabor do Brasil\" (Taste of Brazil)'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the chain and activate verbose\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, verbose = True)\n",
    "\n",
    "# Invoke the chain by passing a parameter to the prompt\n",
    "chain.invoke(\"Brazilian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code creates an instance of LLMChain, a class designed to chain or sequence operations using an LLM. This instance is configured to use a specific language model and a predefined prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a Thai restaurant. Suggest a fancy name for it.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Thai', 'text': '\\n\\n\"Lotus Blossom Thai Bistro\"'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the chain and activate verbose\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, verbose = True)\n",
    "\n",
    "# Invoke the chain by passing a parameter to the prompt\n",
    "chain.invoke(\"Thai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sequential Chain\n",
    "\n",
    "A SimpleSequentialChain in LangChain is a chain type that executes a sequence of components (e.g. LLMs, tools, other chains) in a predefined order. It is one of the most basic and commonly used chain types in LangChain.\n",
    "\n",
    "A sample use case for SimpleSequentialChain could be a question answering system where:\n",
    "\n",
    "- The first component is an LLM that analyzes the input question.\n",
    "- The second component is a tool that retrieves relevant documents from a database.\n",
    "- The third component is another LLM that generates an answer based on the question and the retrieved documents.\n",
    "\n",
    "By chaining these components together into a SimpleSequentialChain, you can create a more complex and capable system while maintaining a modular and extensible architecture.\n",
    "\n",
    "While SimpleSequentialChain is useful for linear workflows, LangChain also provides other chain types such as ConditionalChain and SequentialChain for more complex control flows and branching logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSuggest some menu items for \n",
      "\n",
      "\"Hoosier's Hearth\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Indiana', 'output': '\\n\\n1. Fried Pork Tenderloin Sandwich with Homemade Potato Chips\\n2. Indiana Corn Chowder served in a Bread Bowl\\n3. Hoosier Fried Chicken with Mashed Potatoes and Gravy\\n4. Biscuits and Gravy with Sausage Patties\\n5. Indiana Pork BBQ Ribs with Corn on the Cob\\n6. Sweet Corn Fritters with Maple Syrup\\n7. Indiana Farmhouse Salad with Local Greens and Veggies\\n8. Hoosier Meatloaf with Roasted Root Vegetables\\n9. Indiana Apple Pie with Vanilla Ice Cream\\n10. Hoosier Pork and Beans Casserole\\n11. Indiana Fried Catfish with Hushpuppies\\n12. Hoosier Beef Stew with Cornbread\\n13. Indiana Sweet Potato Casserole with Pecan Streusel Topping\\n14. Hoosier Hoagie Sandwich with Ham, Turkey, and Provolone Cheese\\n15. Indiana Maple Syrup Glazed Ham with Scalloped Potatoes.'}\n"
     ]
    }
   ],
   "source": [
    "# Sets LLM with lower temperature\n",
    "llm = OpenAI(temperature = 0.6)\n",
    "\n",
    "# Create the prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a {cuisine} restaurant. Suggest a fancy name for it.\")\n",
    "\n",
    "\n",
    "# Create the chain\n",
    "chain_1 = LLMChain(llm = llm, prompt = prompt_template_name)\n",
    "\n",
    "\n",
    "# Create another prompt template\n",
    "prompt_template_items = PromptTemplate( input_variables = ['restaurant name'], template = \"\"\"Suggest some menu items for {restaurant_name}\"\"\")\n",
    "\n",
    "\n",
    "# Create the chain\n",
    "chain_2 = LLMChain(llm = llm, prompt = prompt_template_items, verbose=True)\n",
    "\n",
    "\n",
    "# Concatenates the two chains\n",
    "chain_final = SimpleSequentialChain(chains = [chain_1, chain_2])\n",
    "\n",
    "\n",
    "# Invoca a chain\n",
    "print(chain_final.invoke(\"Indiana\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chain\n",
    "\n",
    "SequentialChain is a more advanced version of SimpleSequentialChain. While SimpleSequentialChain executes a fixed sequence of components, SequentialChain allows dynamic and conditional execution of components based on the outputs of previous components.\n",
    "\n",
    "A sample use case for SequentialChain could be a conversational agent that:\n",
    "\n",
    "- Uses an LLM to understand user input and determine the appropriate action.\n",
    "\n",
    "- Conditionally executes different components (e.g., database lookup, API call, calculation) based on the output of the LLM.\n",
    "\n",
    "- Optionally prompts the user for additional information if needed.\n",
    "\n",
    "- Generates a final response using another LLM, based on the outputs of previous components.\n",
    "\n",
    "By leveraging SequentialChain, you can build more intelligent and adaptive applications that can dynamically adjust their behavior based on intermediate results and states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM\n",
    "llm = OpenAI(temperature = 0.7)\n",
    "\n",
    "\n",
    "# Creating the first chain\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a {cuisine} restaurant. Suggest a fancy name for it.\")\n",
    "\n",
    "# Define the chain with an output parameter\n",
    "chain_1 = LLMChain(llm = llm, prompt = prompt_template_name, output_key = \"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the second chain\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "# Define the chain with an output parameter\n",
    "chain_2 = LLMChain(llm = llm, prompt = prompt_template_items, output_key = \"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sequence of chains\n",
    "chain = SequentialChain(chains = [chain_1, chain_2],\n",
    "                        input_variables = ['cuisine'],\n",
    "                        output_variables = ['restaurant_name', \"menu_items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Italian',\n",
       " 'restaurant_name': '\\n\\n\"La Dolce Vita Trattoria\" ',\n",
       " 'menu_items': '\\n\\n1. Antipasto platter with a variety of cured meats, cheeses, olives, and roasted vegetables\\n2. Caprese salad with fresh mozzarella, tomatoes, and basil\\n3. Homemade gnocchi with a choice of marinara, pesto, or creamy alfredo sauce\\n4. Chicken or veal piccata served with lemon-caper sauce\\n5. Seafood risotto with shrimp, scallops, and clams\\n6. Eggplant Parmesan with marinara sauce and melted mozzarella cheese\\n7. Wood-fired pizza with a choice of toppings such as prosciutto, arugula, and truffle oil\\n8. Linguine alle vongole (linguine with clams) in a white wine and garlic sauce\\n9. Osso buco (braised veal shank) with gremolata and creamy polenta\\n10. Tiramisu or cannoli for dessert.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"cuisine\": \"Italian\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuisine: Italian\n",
      "Restaurant Name: \"Bella Vita Trattoria\"\n",
      "Menu Items:\n",
      "1. Antipasto platter: a selection of cured meats, cheeses, olives, and grilled vegetables\n",
      "2. Caprese salad: fresh mozzarella, tomatoes, and basil drizzled with balsamic glaze\n",
      "3. Risotto al Funghi: creamy risotto with wild mushrooms\n",
      "4. Pollo Marsala: chicken breast sautéed with mushrooms and Marsala wine sauce\n",
      "5. Gnocchi al Pesto: potato dumplings in a homemade pesto sauce\n",
      "6. Lasagna Bolognese: layers of pasta, Bolognese sauce, and béchamel cheese\n",
      "7. Linguine Frutti di Mare: linguine pasta with shrimp, scallops, mussels, and clams in a white wine sauce\n",
      "8. Eggplant Parmigiana: breaded eggplant topped with marinara sauce and mozzarella cheese\n",
      "9. Saltimbocca alla Romana: thin sliced veal, prosciutto, and sage in a white wine sauce\n",
      "10. Tiramisu: traditional Italian dessert made with ladyfingers, espresso, and mascarpone cheese.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the method and capturing the response\n",
    "response = chain.invoke({\"cuisine\": \"Italian\"})\n",
    "\n",
    "# Preparing the formatted string\n",
    "formatted_output = f\"Cuisine: {response['cuisine']}\\nRestaurant Name: {response['restaurant_name'].strip()}\\nMenu Items:\"\n",
    "\n",
    "# Adding each menu item to the formatted string\n",
    "menu_items = response['menu_items'].strip().split('\\n')\n",
    "for item in menu_items:\n",
    "    formatted_output += f\"\\n{item}\"\n",
    "\n",
    "# Displaying the formatted output\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Memory for LLM\n",
    "\n",
    "In LangChain, “Memory” refers to components that allow chains, agents, and other constructs to store and retrieve information about previous inputs, outputs, and intermediate states. This allows them to maintain context and make use of relevant information from the history of previous conversations or computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'Mexican', 'text': '\\n\\n\"El Sabor del Sol\" (The Flavor of the Sun)'}\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain\n",
    "name = chain.invoke(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'Argentina', 'text': '\\n\\n\"Tango Gastronomía\"'}\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain\n",
    "name = chain.invoke(\"Argentina\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the memory object\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Casa de Sabor\" \n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, memory = memory)\n",
    "\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Tango Gastronomía\"\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Argentina\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"Casa de Sabor\" \n",
      "Human: Argentina\n",
      "AI: \n",
      "\n",
      "\"Tango Gastronomía\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Chain\n",
    "\n",
    "A ConversationChain is a specialized type of chain designed to handle multi-turn conversations or dialogs with an LLM.\n",
    "\n",
    "A ConversationChain is particularly useful for building conversational agents, chatbots, or any application that requires maintaining context across multiple turns of interaction with a user. By abstracting away the complexities of managing conversation history and formatting prompts, a ConversationChain simplifies the process of building multi-turn dialog systems with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "# Creates the conversation object\n",
    "conv = ConversationChain(llm = OpenAI(temperature = 0.7))\n",
    "\n",
    "print(conv.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Which country has won the Football World Cup the most times?',\n",
       " 'history': '',\n",
       " 'response': ' Brazil has won the Football World Cup the most times, with a total of five wins. They won in 1958, 1962, 1970, 1994, and 2002. They also hold the record for the most consecutive wins, with three victories in a row in 1958, 1962, and 1970. Germany and Italy are tied for second place with four wins each. Germany won in 1954, 1974, 1990, and 2014, while Italy won in 1934, 1938, 1982, and 2006.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.invoke(\"Which country has won the Football World Cup the most times?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is 30 + 12?',\n",
       " 'history': 'Human: Which country has won the Football World Cup the most times?\\nAI:  Brazil has won the Football World Cup the most times, with a total of five wins. They won in 1958, 1962, 1970, 1994, and 2002. They also hold the record for the most consecutive wins, with three victories in a row in 1958, 1962, and 1970. Germany and Italy are tied for second place with four wins each. Germany won in 1954, 1974, 1990, and 2014, while Italy won in 1934, 1938, 1982, and 2006.',\n",
       " 'response': '  30 + 12 is equal to 42.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.invoke(\"What is 30 + 12?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Who is the greatest scorer in the history of the Football World Cup?',\n",
       " 'history': 'Human: Which country has won the Football World Cup the most times?\\nAI:  Brazil has won the Football World Cup the most times, with a total of five wins. They won in 1958, 1962, 1970, 1994, and 2002. They also hold the record for the most consecutive wins, with three victories in a row in 1958, 1962, and 1970. Germany and Italy are tied for second place with four wins each. Germany won in 1954, 1974, 1990, and 2014, while Italy won in 1934, 1938, 1982, and 2006.\\nHuman: What is 30 + 12?\\nAI:   30 + 12 is equal to 42.',\n",
       " 'response': ' The greatest scorer in the history of the Football World Cup is Miroslav Klose from Germany. He has scored a total of 16 goals in four World Cup tournaments (2002, 2006, 2010, and 2014). He surpassed the previous record holder, Brazilian player Ronaldo, who had 15 goals. Klose also holds the record for the most goals scored in a single World Cup tournament, with 5 goals in 2006. He retired from international football in 2014.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.invoke(\"Who is the greatest scorer in the history of the Football World Cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What was the first question I asked?',\n",
       " 'history': 'Human: Which country has won the Football World Cup the most times?\\nAI:  Brazil has won the Football World Cup the most times, with a total of five wins. They won in 1958, 1962, 1970, 1994, and 2002. They also hold the record for the most consecutive wins, with three victories in a row in 1958, 1962, and 1970. Germany and Italy are tied for second place with four wins each. Germany won in 1954, 1974, 1990, and 2014, while Italy won in 1934, 1938, 1982, and 2006.\\nHuman: What is 30 + 12?\\nAI:   30 + 12 is equal to 42.\\nHuman: Who is the greatest scorer in the history of the Football World Cup?\\nAI:  The greatest scorer in the history of the Football World Cup is Miroslav Klose from Germany. He has scored a total of 16 goals in four World Cup tournaments (2002, 2006, 2010, and 2014). He surpassed the previous record holder, Brazilian player Ronaldo, who had 15 goals. Klose also holds the record for the most goals scored in a single World Cup tournament, with 5 goals in 2006. He retired from international football in 2014.',\n",
       " 'response': ' I am an AI and do not have the ability to remember previous questions. My purpose is to provide information and assist in tasks to the best of my abilities.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.invoke(\"What was the first question I asked?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Which country has won the Football World Cup the most times?\n",
      "AI:  Brazil has won the Football World Cup the most times, with a total of five wins. They won in 1958, 1962, 1970, 1994, and 2002. They also hold the record for the most consecutive wins, with three victories in a row in 1958, 1962, and 1970. Germany and Italy are tied for second place with four wins each. Germany won in 1954, 1974, 1990, and 2014, while Italy won in 1934, 1938, 1982, and 2006.\n",
      "Human: What is 30 + 12?\n",
      "AI:   30 + 12 is equal to 42.\n",
      "Human: Who is the greatest scorer in the history of the Football World Cup?\n",
      "AI:  The greatest scorer in the history of the Football World Cup is Miroslav Klose from Germany. He has scored a total of 16 goals in four World Cup tournaments (2002, 2006, 2010, and 2014). He surpassed the previous record holder, Brazilian player Ronaldo, who had 15 goals. Klose also holds the record for the most goals scored in a single World Cup tournament, with 5 goals in 2006. He retired from international football in 2014.\n",
      "Human: What was the first question I asked?\n",
      "AI:  I am an AI and do not have the ability to remember previous questions. My purpose is to provide information and assist in tasks to the best of my abilities.\n"
     ]
    }
   ],
   "source": [
    "print(conv.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conversation Buffer Window Memory\n",
    "\n",
    "ConversationBufferWindowMemory is a type of LangChain memory component designed specifically for use with ConversationChain. It provides a way to store and retrieve conversation history while limiting the amount of context retained based on a specified window size.\n",
    "\n",
    "The main advantage of ConversationBufferWindowMemory is its ability to limit the amount of context provided to the LLM, which can be important for performance and to prevent the model from becoming overwhelmed with too much irrelevant information. By adjusting the window size, you can control the tradeoff between providing enough context and avoiding excessive computational overhead.\n",
    "\n",
    "This type of memory is particularly useful for building conversational agents, chatbots, or any application that requires maintaining a continuous window of relevant recent conversation history for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the memory window\n",
    "memory = ConversationBufferWindowMemory(k = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversation chain\n",
    "conv = ConversationChain(llm = OpenAI(temperature = 0.7), memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The first Football World Cup was won by Uruguay in 1930. They defeated Argentina in the final match with a score of 4-2. The tournament was held in Uruguay and was organized by FIFA. It was a 13-team competition, with France, Belgium, Yugoslavia, Romania, Brazil, Bolivia, Peru, Chile, Paraguay, and Mexico also participating. Uruguay's victory was hailed as a triumph for the host nation, and the tournament was seen as a success overall. Are there any other questions you have about the history of the Football World Cup?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke LLM\n",
    "conv.run(\"Who won the first Football World Cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is 10 + 19?',\n",
       " 'history': \"Human: Who won the first Football World Cup?\\nAI:  The first Football World Cup was won by Uruguay in 1930. They defeated Argentina in the final match with a score of 4-2. The tournament was held in Uruguay and was organized by FIFA. It was a 13-team competition, with France, Belgium, Yugoslavia, Romania, Brazil, Bolivia, Peru, Chile, Paraguay, and Mexico also participating. Uruguay's victory was hailed as a triumph for the host nation, and the tournament was seen as a success overall. Are there any other questions you have about the history of the Football World Cup?\",\n",
       " 'response': '  10 + 19 is equal to 29. Is there anything else you would like to know?'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke LLM\n",
    "conv.invoke(\"What is 10 + 19?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Who was the captain of the winning team of the first Football World Cup?',\n",
       " 'history': 'Human: What is 10 + 19?\\nAI:   10 + 19 is equal to 29. Is there anything else you would like to know?',\n",
       " 'response': ' The first Football World Cup was held in 1930 in Uruguay. The winning team was Uruguay, and their captain was José Nasazzi. He was a defender and played for the Uruguayan club Nacional. Is there anything else you would like to know?'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke LLM\n",
    "conv.invoke(\"Who was the captain of the winning team of the first Football World Cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who was the captain of the winning team of the first Football World Cup?\n",
      "AI:  The first Football World Cup was held in 1930 in Uruguay. The winning team was Uruguay, and their captain was José Nasazzi. He was a defender and played for the Uruguayan club Nacional. Is there anything else you would like to know?\n"
     ]
    }
   ],
   "source": [
    "print(conv.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain and VectorDB for Web Scraping\n",
    "\n",
    "ChromaDB is a vector storage database library that integrates with LangChain. It provides functionality for efficiently storing, retrieving, and searching large amounts of text data using vector embeddings and semantic similarity.\n",
    "\n",
    "https://www.trychroma.com/\n",
    "\n",
    "https://pypi.org/project/chromadb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web data extraction\n",
    "data = WebBaseLoader(\n",
    "    # \"https://www.nzherald.co.nz/world/donald-trumps-tariff-threat-forces-colombia-to-accept-deportees-on-military-planes/7WA36M4OQBFYVHM4XLAUAN2BE4/\"\n",
    "    \"https://www.bbc.com/future/article/20250124-how-to-carry-more-than-your-own-bodyweight\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Always check a website's robots.txt before scraping data. Don't scrape if it's not allowed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "documents = data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first document (in this case there is only one document)\n",
    "document = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'metadata', 'page_content', 'type'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary keys\n",
    "document.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Springy poles and forehead straps: How to carry more than your own bodyweightSkip to contentBritish '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 100 characters\n",
    "document.page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://www.bbc.com/future/article/20250124-how-to-carry-more-than-your-own-bodyweight',\n",
       " 'title': 'Springy poles and forehead straps: How to carry more than your own bodyweight',\n",
       " 'description': \"Some communities have developed techniques to help them carry heavier loads. Here's what we can learn from them.\",\n",
       " 'language': 'en-GB'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metadata\n",
    "document.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print formatted result\n",
    "def print_response(response: str):\n",
    "    print(\"\\n\".join(textwrap.wrap(response, width = 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Some communities, such as the rural farm workers of Vietnam, have developed techniques to help them carry heavier loads. These techniques include using springy poles and forehead straps to distribute the weight and make it easier to carry. Additionally, some engineers have developed spring-loaded or \"floating\" backpacks to ease the force of loads on the back and shoulders. Military personnel also often carry loads that exceed their own bodyweight, and have developed techniques and equipment to make it easier, such as using straps and distributing the weight evenly. The sherpa method of carrying heavy loads has also been studied and found to be a combination of weight training and cardio, which helps build endurance and strength for carrying heavy loads.\n",
      "\n",
      "Source Documents:\n",
      " Some communities, such as the rural farm workers of Vietnam, have developed techniques to help them\n",
      "carry heavier loads. These techniques include using springy poles and forehead straps to distribute\n",
      "the weight and make it easier to carry. Additionally, some engineers have developed spring-loaded or\n",
      "\"floating\" backpacks to ease the force of loads on the back and shoulders. Military personnel also\n",
      "often carry loads that exceed their own bodyweight, and have developed techniques and equipment to\n",
      "make it easier, such as using straps and distributing the weight evenly. The sherpa method of\n",
      "carrying heavy loads has also been studied and found to be a combination of weight training and\n",
      "cardio, which helps build endurance and strength for carrying heavy loads.\n"
     ]
    }
   ],
   "source": [
    "# Create the index using VectorstoreIndexCreator\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    embedding=OpenAIEmbeddings(),  # Define the embeddings\n",
    ")\n",
    "vectorstore = index_creator.from_loaders([data])  # Create the index from the loader\n",
    "\n",
    "# Define the LLM for querying\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.vectorstore.as_retriever(),  # Use the retriever from the vectorstore\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# Query the index\n",
    "query = \"How to carry more than your own bodyweight?\"\n",
    "response = retrieval_chain({\"query\": query})\n",
    "\n",
    "# Print the result and sources\n",
    "print(\"Answer:\", response[\"result\"])\n",
    "print(\"\\nSource Documents:\")\n",
    "\n",
    "print_response(response=response['result'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work with VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template\n",
    "template = \"\"\"\n",
    "{context}\n",
    "\n",
    "Please answer considering the most modern methods you know.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "prompt = PromptTemplate(template = template, input_variables = [\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You're a specialist\n",
      "\n",
      "Please answer considering the most modern methods you know.\n",
      "\n",
      "Question: How can I become stronger?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Print the prompt to view the format\n",
    "print(prompt.format(\n",
    "    context = \"You're a specialist\",\n",
    "    question = \"How can I become stronger?\",)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings object\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code refers to initializing an instance of the OpenAIEmbeddings class, which is an interface for generating embeddings (vector representations) using models provided by OpenAI, such as the GPT language models. Embeddings are transformations of raw data, such as text, into vectors of fixed numbers, capturing semantic and contextual aspects of the original content in a way that can be processed by machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates VectorDB by converting text documents into numeric representations (embeddings)\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code is for creating a vector database using Chroma. This operation involves preparing a data structure optimized for searches and analysis based on documents and their respective embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(db)\n",
    "\n",
    "# Arguments\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "# Chain de RetrievalQA\n",
    "chain = RetrievalQA.from_chain_type(llm = ChatOpenAI(temperature = 0),\n",
    "                                    chain_type = \"stuff\",\n",
    "                                    retriever = db.as_retriever(search_kwargs = {\"k\": 1}),\n",
    "                                    chain_type_kwargs = chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code is creating an instance called chain, using the RetrievalQA class to configure a process chain focused on performing Question Answering (QA) tasks based on information retrieval. The from_chain_type method is used to specify the type of process chain and configure its main components, such as the language model and the retrieval engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What makes a person stronger, according to the text?', 'result': 'According to the text, a person can become stronger by engaging in strength training, building muscle mass, and incorporating progressive weight training into their exercise routine. Additionally, combining cardio and strength training, as well as focusing on technique and gradually increasing loads, can help improve strength and carry heavier loads.'}\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query = \"What makes a person stronger, according to the text?\"\n",
    "\n",
    "# Response\n",
    "response = chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building Sales Expert Chatbot with LangChain and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining LLM\n",
    "gpt = ChatOpenAI(temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template\n",
    "template = \"\"\"This is a conversation between a customer and a sports car sales specialist.\n",
    "\n",
    "You are the car specialist, you know sports cars well and you should always answer\n",
    "as accurately as possible.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "CarSpecialist:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt template\n",
    "prompt = PromptTemplate(input_variables = [\"history\", \"input\"], template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversation chain\n",
    "conversation = ConversationChain(prompt = prompt,\n",
    "                                 llm = gpt,\n",
    "                                 verbose = False,\n",
    "                                 memory = ConversationBufferMemory(ai_prefix = \"CarSpecialist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expert: Hello! How can I assist you today?\n",
      "\n",
      "\n",
      "Expert: I'm looking to purchase a sports car. Can you recommend any models that are known for their\n",
      "performance and handling?\n",
      "\n",
      "\n",
      "Expert: Absolutely! Some popular sports car models known for their performance and handling are the\n",
      "Porsche 911, Chevrolet Corvette, BMW M3, and Nissan GT-R. Each of these cars offers a unique driving\n",
      "experience and top-notch performance capabilities. Do you have a specific budget or preference in\n",
      "mind?\n",
      "\n",
      "\n",
      "Expert: I'm looking for something with a budget of around $50,000. Do you have any recommendations\n",
      "within that price range?\n",
      "\n",
      "\n",
      "Expert: For a budget of around $50,000, I would recommend looking into the Ford Mustang GT, Subaru\n",
      "WRX STI, or the Chevrolet Camaro SS. These models offer great performance and handling at a more\n",
      "affordable price point. Would you like to schedule a test drive or learn more about any of these\n",
      "options?\n",
      "\n",
      "\n",
      "Thanks for Using the AI-Based Customer Service System!\n"
     ]
    }
   ],
   "source": [
    "# Conversation loop limited to 5 interactions (increase the number of interactions or remove the if block)\n",
    "\n",
    "# Initialize the counter\n",
    "counter = 0\n",
    "\n",
    "# Loop\n",
    "while True:\n",
    "\n",
    "    prompt = input(prompt = \"Customer: \")\n",
    "    print()\n",
    "    result = conversation(prompt)\n",
    "    print_response(\"Expert: \" + result[\"response\"])\n",
    "    print()\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    if counter >= 5:\n",
    "        print('\\nThanks for Using the AI-Based Customer Service System!')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
