{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoAssistAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import chromadb\n",
    "import langchain\n",
    "import sqlalchemy\n",
    "import langchain_openai\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting to Know LangChain**\n",
    "\n",
    "[Oficial documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "\n",
    "LangChain is a powerful framework designed to simplify the development of applications that use language models (LLMs). Its modular structure and versatility allow developers to build a wide range of solutions, from simple automation tasks to complex systems like chatbots, question-and-answer platforms, and more.\n",
    "\n",
    "---\n",
    "\n",
    "**What is LangChain?**  \n",
    "LangChain is an open-source library that bridges the gap between LLMs and real-world applications by enabling seamless integration with various tools, data sources, and workflows. Its goal is to simplify the development process while offering robust capabilities for building intelligent applications.\n",
    "\n",
    "---\n",
    "\n",
    "**Core Features of LangChain**\n",
    "\n",
    "1. **Modularity and Customization**  \n",
    "   LangChain's modular design allows developers to integrate components like LLMs, prompt templates, memory, and agents. Each component can be customized to meet specific requirements, making the framework flexible and versatile.\n",
    "\n",
    "2. **Integration with External Data**  \n",
    "   One of LangChain's key features is Retrieval-Augmented Generation (RAG), enabling applications to retrieve and use external data sources like web content, documents, or APIs to provide more accurate and context-aware responses.\n",
    "\n",
    "3. **Memory Management**  \n",
    "   LangChain provides various types of memory, such as buffer memory and conversation memory, allowing applications to maintain context and improve user interactions over time.\n",
    "\n",
    "4. **Agent Framework**  \n",
    "   LangChain supports agents capable of dynamically deciding which tools or APIs to use based on user inputs, adding a layer of intelligence to your applications.\n",
    "\n",
    "5. **Wide Compatibility**  \n",
    "   It works seamlessly with a variety of LLMs, such as OpenAI's GPT models, Hugging Face transformers, and custom fine-tuned models, ensuring flexibility in choosing the best model for your use case.\n",
    "\n",
    "---\n",
    "\n",
    "**Applications of LangChain**\n",
    "\n",
    "- **Chatbots**: Create intelligent and context-aware conversational agents.  \n",
    "- **Question-Answer Systems**: Build systems capable of answering domain-specific questions using RAG and external data.  \n",
    "- **Automated Processes**: Develop tools for summarizing, translating, or analyzing text data.  \n",
    "- **Custom LLM Solutions**: Fine-tune language models with LangChain to address unique business problems.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Use LangChain?**\n",
    "\n",
    "LangChain simplifies the integration of language models with external tools and data sources, accelerating the development of sophisticated AI-driven applications. Whether you're building a chatbot, a data-powered assistant, or a customized LLM, LangChain offers the tools and flexibility to bring your ideas to life.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Diving Deeper into LangChain Components**\n",
    "\n",
    "LangChain’s architecture is built around several core components, each designed to perform a specific function that simplifies the integration and application of large language models (LLMs). Below, we’ll explore these components in detail:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Models**  \n",
    "The **model** is the heart of LangChain. It interacts with the language model (LLM) to generate predictions, completions, or responses.\n",
    "\n",
    "- **Supported Models:**  \n",
    "  LangChain supports a wide range of LLMs, including:\n",
    "  - OpenAI's GPT (e.g., GPT-3.5, GPT-4).\n",
    "  - Hugging Face Transformers.\n",
    "  - Open-source models (e.g., Llama, BLOOM, Falcon).\n",
    "  \n",
    "- **Customization:**  \n",
    "  Developers can fine-tune models, adjust hyperparameters, and incorporate specialized pre-trained models for domain-specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Prompts**  \n",
    "Prompts define how input is structured and presented to the LLM. Crafting effective prompts is crucial for achieving accurate and relevant responses.\n",
    "\n",
    "- **Prompt Templates:**  \n",
    "  LangChain provides tools for creating reusable templates with placeholders for dynamic inputs.  \n",
    "  Example:  \n",
    "  ```python\n",
    "  from langchain.prompts import PromptTemplate\n",
    "\n",
    "  prompt = PromptTemplate(\n",
    "      input_variables=[\"context\", \"question\"],\n",
    "      template=\"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
    "  )\n",
    "  ```\n",
    "  \n",
    "- **Prompt Optimization:**  \n",
    "  LangChain facilitates testing and iteration of prompts to maximize model performance.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Memory**  \n",
    "Memory allows the system to retain information between interactions, making applications context-aware.\n",
    "\n",
    "- **Types of Memory:**  \n",
    "  - **ConversationBufferMemory:** Stores the entire conversation history.  \n",
    "  - **ConversationSummaryMemory:** Summarizes past interactions to maintain context efficiently.  \n",
    "  - **VectorStoreRetrieverMemory:** Uses embeddings to retrieve relevant context dynamically.\n",
    "\n",
    "- **Use Case:**  \n",
    "  For chatbots, memory ensures that the bot understands and maintains context throughout a conversation.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Chains**  \n",
    "Chains are sequences of operations that transform inputs into outputs. LangChain allows developers to build complex workflows by chaining multiple components together.\n",
    "\n",
    "- **LLMChain:**  \n",
    "  The simplest type of chain, consisting of a prompt and an LLM.  \n",
    "  Example:  \n",
    "  ```python\n",
    "  from langchain.chains import LLMChain\n",
    "  from langchain.llms import OpenAI\n",
    "\n",
    "  llm = OpenAI(model=\"gpt-4\")\n",
    "  chain = LLMChain(llm=llm, prompt=prompt)\n",
    "  response = chain.run({\"context\": \"AI is transforming industries.\", \"question\": \"How is it used in healthcare?\"})\n",
    "  ```\n",
    "  \n",
    "- **Sequential Chains:**  \n",
    "  Combine multiple chains to perform more complex tasks, such as summarization followed by question-answering.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Tools and Agents**  \n",
    "Agents are decision-makers that dynamically decide which tools to use based on user input. Tools provide external capabilities, such as searching the web or accessing APIs.\n",
    "\n",
    "- **Tools:**  \n",
    "  Common tools include:\n",
    "  - **Web Search:** Retrieve real-time information.\n",
    "  - **Calculators:** Perform mathematical computations.\n",
    "  - **Databases:** Query structured or unstructured data.\n",
    "\n",
    "- **Agents:**  \n",
    "  Agents use prompts to decide which tool to invoke and how to handle responses.  \n",
    "  Example: An agent might search the web for information if a question cannot be answered using the LLM alone.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Data Connectors**  \n",
    "LangChain supports **Retrieval-Augmented Generation (RAG)** by integrating with external data sources. This makes LLMs more powerful and capable of providing accurate, context-specific answers.\n",
    "\n",
    "- **Data Sources:**  \n",
    "  - **Vector Databases:** Pinecone, Weaviate, FAISS.  \n",
    "  - **Document Loaders:** PDFs, Excel files, web scraping.  \n",
    "  - **APIs:** Integrate third-party APIs for live data retrieval.\n",
    "\n",
    "- **Embedding Models:**  \n",
    "  LangChain allows embeddings to be generated for indexing and searching data. This ensures relevant information is retrieved efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Evaluation**  \n",
    "LangChain includes tools for evaluating and debugging applications to ensure they meet performance requirements.\n",
    "\n",
    "- **Human-in-the-Loop (HITL):**  \n",
    "  Involve human evaluators to assess the quality of responses.  \n",
    "- **Automated Evaluation:**  \n",
    "  Use metrics like BLEU, ROUGE, or accuracy to measure performance.\n",
    "\n",
    "---\n",
    "\n",
    "**8. Deployment**  \n",
    "LangChain applications can be deployed on various platforms, making them scalable and production-ready.\n",
    "\n",
    "- **Cloud Platforms:** AWS, GCP, Azure.  \n",
    "- **Dockerization:** Containerize LangChain apps for easy deployment.  \n",
    "- **Integration with APIs:** Expose the functionality as RESTful APIs for external use.\n",
    "\n",
    "---\n",
    "\n",
    "**9. Advanced Features**  \n",
    "- **Streaming:** LangChain supports streaming responses for real-time applications like live chat interfaces.  \n",
    "- **Callbacks:** Monitor and log the internal workflow of chains and agents for debugging or tracking.\n",
    "\n",
    "---\n",
    "\n",
    "**Why These Components Matter**  \n",
    "Each component is modular and can be independently configured, allowing developers to:\n",
    "- Customize solutions for specific use cases.\n",
    "- Scale applications without overhauling existing structures.\n",
    "- Ensure high performance and efficiency by leveraging the best tools and integrations.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to focus on a specific component, or provide an example project that ties these components together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Defining the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the API key\n",
    "with open('../ignore/secret_key.json') as f:\n",
    "    os.environ['OPENAI_API_KEY'] = json.load(f)['secret_key']\n",
    "    \n",
    "\n",
    "# Defines the LLM\n",
    "# Creates an instance of a Large Language Model (LLM), specifically one provided by OpenAI\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature is a hyperparameter that influences the randomness of the responses generated by the model. A higher temperature value (usually ranging from 0 to 1) promotes more creative and varied responses. On the other hand, a lower temperature tends to cause the model to produce more deterministic and possibly more predictable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Sakura Garden Dining\" \n"
     ]
    }
   ],
   "source": [
    "# Send the prompt to LLM and capture the response\n",
    "nome = llm.invoke(\"I want to open a Japanese food restaurant. Suggest a fancy name for it.\")\n",
    "print(nome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, the string “I want to open a Japanese food restaurant. Suggest a fancy name for it.” serves as the prompt or input to the language model. It describes the task the user wants the model to perform: creatively generating a name for a new Japanese food restaurant. The model will use its natural language training and prior knowledge to generate a response that meets this request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates in the context of LangChain refer to structured ways of formatting input to large language models (LLMs) to improve their performance and adherence to desired behaviors.\n",
    "\n",
    "A prompt template defines a template sequence with placeholder variables that can be populated dynamically. This allows you to construct prompts in a consistent and programmatic manner, rather than hard-coding full prompts.\n",
    "\n",
    "Prompt templates in LangChain provide a structured and extensible way to interface with LLMs, making it easy to explore and optimize prompting strategies to improve language model performance on specific tasks or domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a {cuisine} restaurant. Suggest a fancy name for it.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code defines a PromptTemplate, a framework that allows you to create dynamic prompts for use with Large Language Models (LLMs). This approach is particularly useful when you want to generate custom prompts based on specific variables or when you want to reuse a prompt format with different data sets.\n",
    "\n",
    "**input_variables = ['cuisine']**: Defines a list of variables that can be used to populate the template. In this case, there is a single variable called 'cuisine'. This variable acts as a placeholder that will be replaced with a specific value when the template is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a Mexican restaurant. Suggest a fancy name for it.\n"
     ]
    }
   ],
   "source": [
    "# Use the previously defined template to generate a specific prompt,\n",
    "# inserting the value \"Italiana\" in place of the variable culinary\n",
    "p = prompt_template_name.format(cuisine = \"Mexican\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Sequences with LLMChain\n",
    "\n",
    "Chains in LangChain are sequences of operations that can process inputs and generate outputs by combining multiple components, including large language models (LLMs), other chains, and specialized tools or utilities.\n",
    "\n",
    "An LLMChain is a type of chain that allows you to interact with a large language model (LLM) in a structured way. It provides a simple interface for passing inputs to the LLM and retrieving its outputs.\n",
    "\n",
    "The LLMChain serves as a building block for many other constructs in LangChain, such as agents, tools, and more advanced chain types. By encapsulating the LLM interaction logic in a reusable and extensible component, LLMChain simplifies the process of building applications that leverage large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a Brazilian restaurant. Suggest a fancy name for it.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Brazilian',\n",
       " 'text': '\\n\\n\"Sabor do Brasil\" which means \"Taste of Brazil\" in Portuguese.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the chain and activate verbose\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, verbose = True)\n",
    "\n",
    "# Invoke the chain by passing a parameter to the prompt\n",
    "chain.invoke(\"Brazilian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code creates an instance of LLMChain, a class designed to chain or sequence operations using an LLM. This instance is configured to use a specific language model and a predefined prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a Thai restaurant. Suggest a fancy name for it.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Thai', 'text': '\\n\\n\"Silk & Spice Thai Bistro\"'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the chain and activate verbose\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, verbose = True)\n",
    "\n",
    "# Invoke the chain by passing a parameter to the prompt\n",
    "chain.invoke(\"Thai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sequential Chain\n",
    "\n",
    "A SimpleSequentialChain in LangChain is a chain type that executes a sequence of components (e.g. LLMs, tools, other chains) in a predefined order. It is one of the most basic and commonly used chain types in LangChain.\n",
    "\n",
    "A sample use case for SimpleSequentialChain could be a question answering system where:\n",
    "\n",
    "- The first component is an LLM that analyzes the input question.\n",
    "- The second component is a tool that retrieves relevant documents from a database.\n",
    "- The third component is another LLM that generates an answer based on the question and the retrieved documents.\n",
    "\n",
    "By chaining these components together into a SimpleSequentialChain, you can create a more complex and capable system while maintaining a modular and extensible architecture.\n",
    "\n",
    "While SimpleSequentialChain is useful for linear workflows, LangChain also provides other chain types such as ConditionalChain and SequentialChain for more complex control flows and branching logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSuggest some menu items for \n",
      "\n",
      "\"Hoosier Harvest Bistro\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Indiana', 'output': '\\n\\n1. Indiana Corn Chowder\\n2. Hoosier Fried Pork Tenderloin Sandwich\\n3. Maple-Glazed Pork Chop with Apple Chutney\\n4. Grilled Indiana Rainbow Trout with Lemon Butter Sauce\\n5. Hoosier Beef and Noodles\\n6. Maple-Bacon Brussels Sprouts\\n7. Hoosier Fried Chicken and Waffles\\n8. Sweet Corn Fritters with Honey Butter\\n9. Indiana Pork BBQ Ribs\\n10. Hoosier Apple Pie with Cinnamon Ice Cream.'}\n"
     ]
    }
   ],
   "source": [
    "# Sets LLM with lower temperature\n",
    "llm = OpenAI(temperature = 0.6)\n",
    "\n",
    "# Create the prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a {cuisine} restaurant. Suggest a fancy name for it.\")\n",
    "\n",
    "\n",
    "# Create the chain\n",
    "chain_1 = LLMChain(llm = llm, prompt = prompt_template_name)\n",
    "\n",
    "\n",
    "# Create another prompt template\n",
    "prompt_template_items = PromptTemplate( input_variables = ['restaurant name'], template = \"\"\"Suggest some menu items for {restaurant_name}\"\"\")\n",
    "\n",
    "\n",
    "# Create the chain\n",
    "chain_2 = LLMChain(llm = llm, prompt = prompt_template_items, verbose=True)\n",
    "\n",
    "\n",
    "# Concatenates the two chains\n",
    "chain_final = SimpleSequentialChain(chains = [chain_1, chain_2])\n",
    "\n",
    "\n",
    "# Invoca a chain\n",
    "print(chain_final.invoke(\"Indiana\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chain\n",
    "\n",
    "SequentialChain is a more advanced version of SimpleSequentialChain. While SimpleSequentialChain executes a fixed sequence of components, SequentialChain allows dynamic and conditional execution of components based on the outputs of previous components.\n",
    "\n",
    "A sample use case for SequentialChain could be a conversational agent that:\n",
    "\n",
    "- Uses an LLM to understand user input and determine the appropriate action.\n",
    "\n",
    "- Conditionally executes different components (e.g., database lookup, API call, calculation) based on the output of the LLM.\n",
    "\n",
    "- Optionally prompts the user for additional information if needed.\n",
    "\n",
    "- Generates a final response using another LLM, based on the outputs of previous components.\n",
    "\n",
    "By leveraging SequentialChain, you can build more intelligent and adaptive applications that can dynamically adjust their behavior based on intermediate results and states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM\n",
    "llm = OpenAI(temperature = 0.7)\n",
    "\n",
    "\n",
    "# Creating the first chain\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a {cuisine} restaurant. Suggest a fancy name for it.\")\n",
    "\n",
    "# Define the chain with an output parameter\n",
    "chain_1 = LLMChain(llm = llm, prompt = prompt_template_name, output_key = \"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the second chain\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "# Define the chain with an output parameter\n",
    "chain_2 = LLMChain(llm = llm, prompt = prompt_template_items, output_key = \"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sequence of chains\n",
    "chain = SequentialChain(chains = [chain_1, chain_2],\n",
    "                        input_variables = ['cuisine'],\n",
    "                        output_variables = ['restaurant_name', \"menu_items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Italian',\n",
       " 'restaurant_name': '\\n\\n\"Bella Cucina\"',\n",
       " 'menu_items': '\\n1. Antipasto Platter: A selection of cured meats, cheeses, olives, and marinated vegetables\\n2. Caprese Salad: Fresh mozzarella, tomatoes, and basil drizzled with balsamic glaze\\n3. Lasagna Bolognese: Layers of homemade pasta, meat sauce, and creamy bechamel\\n4. Chicken Piccata: Pan-seared chicken in a lemon butter sauce with capers\\n5. Shrimp Scampi: Sautéed shrimp in a garlic white wine sauce served over linguine\\n6. Eggplant Parmigiana: Breaded and fried eggplant topped with marinara and melted mozzarella\\n7. Gnocchi al Pesto: Homemade potato dumplings in a basil pesto sauce\\n8. Margherita Pizza: Fresh tomato, basil, and mozzarella on a thin crust\\n9. Fettuccine Alfredo: Creamy parmesan sauce tossed with fettuccine pasta\\n10. Tiramisu: Traditional Italian dessert made with ladyfingers, coffee, and mascarpone cheese.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"cuisine\": \"Italian\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuisine: Italian\n",
      "Restaurant Name: \"La Bella Cucina\"\n",
      "Menu Items:\n",
      "1. Traditional Italian antipasto platter with cured meats, cheeses, olives, and marinated vegetables\n",
      "2. Homemade pasta dishes including lasagna, ravioli, and spaghetti carbonara\n",
      "3. Classic margherita pizza with fresh mozzarella, tomatoes, and basil\n",
      "4. Chicken or veal marsala with a rich mushroom and marsala wine sauce\n",
      "5. Grilled salmon with lemon and herb butter served with roasted vegetables\n",
      "6. Eggplant parmesan topped with melted mozzarella and served with a side of spaghetti marinara\n",
      "7. Caprese salad with ripe tomatoes, fresh mozzarella, and balsamic glaze\n",
      "8. Risotto with wild mushrooms, truffle oil, and parmesan cheese\n",
      "9. Meatballs in a homemade tomato sauce served with crusty garlic bread\n",
      "10. Tiramisu or cannolis for dessert.\n"
     ]
    }
   ],
   "source": [
    "# Invoking the method and capturing the response\n",
    "response = chain.invoke({\"cuisine\": \"Italian\"})\n",
    "\n",
    "# Preparing the formatted string\n",
    "formatted_output = f\"Cuisine: {response['cuisine']}\\nRestaurant Name: {response['restaurant_name'].strip()}\\nMenu Items:\"\n",
    "\n",
    "# Adding each menu item to the formatted string\n",
    "menu_items = response['menu_items'].strip().split('\\n')\n",
    "for item in menu_items:\n",
    "    formatted_output += f\"\\n{item}\"\n",
    "\n",
    "# Displaying the formatted output\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
