{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoAssistAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import chromadb\n",
    "import langchain\n",
    "import sqlalchemy\n",
    "import langchain_openai\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting to Know LangChain**\n",
    "\n",
    "[Oficial documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "\n",
    "LangChain is a powerful framework designed to simplify the development of applications that use language models (LLMs). Its modular structure and versatility allow developers to build a wide range of solutions, from simple automation tasks to complex systems like chatbots, question-and-answer platforms, and more.\n",
    "\n",
    "---\n",
    "\n",
    "**What is LangChain?**  \n",
    "LangChain is an open-source library that bridges the gap between LLMs and real-world applications by enabling seamless integration with various tools, data sources, and workflows. Its goal is to simplify the development process while offering robust capabilities for building intelligent applications.\n",
    "\n",
    "---\n",
    "\n",
    "**Core Features of LangChain**\n",
    "\n",
    "1. **Modularity and Customization**  \n",
    "   LangChain's modular design allows developers to integrate components like LLMs, prompt templates, memory, and agents. Each component can be customized to meet specific requirements, making the framework flexible and versatile.\n",
    "\n",
    "2. **Integration with External Data**  \n",
    "   One of LangChain's key features is Retrieval-Augmented Generation (RAG), enabling applications to retrieve and use external data sources like web content, documents, or APIs to provide more accurate and context-aware responses.\n",
    "\n",
    "3. **Memory Management**  \n",
    "   LangChain provides various types of memory, such as buffer memory and conversation memory, allowing applications to maintain context and improve user interactions over time.\n",
    "\n",
    "4. **Agent Framework**  \n",
    "   LangChain supports agents capable of dynamically deciding which tools or APIs to use based on user inputs, adding a layer of intelligence to your applications.\n",
    "\n",
    "5. **Wide Compatibility**  \n",
    "   It works seamlessly with a variety of LLMs, such as OpenAI's GPT models, Hugging Face transformers, and custom fine-tuned models, ensuring flexibility in choosing the best model for your use case.\n",
    "\n",
    "---\n",
    "\n",
    "**Applications of LangChain**\n",
    "\n",
    "- **Chatbots**: Create intelligent and context-aware conversational agents.  \n",
    "- **Question-Answer Systems**: Build systems capable of answering domain-specific questions using RAG and external data.  \n",
    "- **Automated Processes**: Develop tools for summarizing, translating, or analyzing text data.  \n",
    "- **Custom LLM Solutions**: Fine-tune language models with LangChain to address unique business problems.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Use LangChain?**\n",
    "\n",
    "LangChain simplifies the integration of language models with external tools and data sources, accelerating the development of sophisticated AI-driven applications. Whether you're building a chatbot, a data-powered assistant, or a customized LLM, LangChain offers the tools and flexibility to bring your ideas to life.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Diving Deeper into LangChain Components**\n",
    "\n",
    "LangChain’s architecture is built around several core components, each designed to perform a specific function that simplifies the integration and application of large language models (LLMs). Below, we’ll explore these components in detail:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Models**  \n",
    "The **model** is the heart of LangChain. It interacts with the language model (LLM) to generate predictions, completions, or responses.\n",
    "\n",
    "- **Supported Models:**  \n",
    "  LangChain supports a wide range of LLMs, including:\n",
    "  - OpenAI's GPT (e.g., GPT-3.5, GPT-4).\n",
    "  - Hugging Face Transformers.\n",
    "  - Open-source models (e.g., Llama, BLOOM, Falcon).\n",
    "  \n",
    "- **Customization:**  \n",
    "  Developers can fine-tune models, adjust hyperparameters, and incorporate specialized pre-trained models for domain-specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Prompts**  \n",
    "Prompts define how input is structured and presented to the LLM. Crafting effective prompts is crucial for achieving accurate and relevant responses.\n",
    "\n",
    "- **Prompt Templates:**  \n",
    "  LangChain provides tools for creating reusable templates with placeholders for dynamic inputs.  \n",
    "  Example:  \n",
    "  ```python\n",
    "  from langchain.prompts import PromptTemplate\n",
    "\n",
    "  prompt = PromptTemplate(\n",
    "      input_variables=[\"context\", \"question\"],\n",
    "      template=\"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
    "  )\n",
    "  ```\n",
    "  \n",
    "- **Prompt Optimization:**  \n",
    "  LangChain facilitates testing and iteration of prompts to maximize model performance.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Memory**  \n",
    "Memory allows the system to retain information between interactions, making applications context-aware.\n",
    "\n",
    "- **Types of Memory:**  \n",
    "  - **ConversationBufferMemory:** Stores the entire conversation history.  \n",
    "  - **ConversationSummaryMemory:** Summarizes past interactions to maintain context efficiently.  \n",
    "  - **VectorStoreRetrieverMemory:** Uses embeddings to retrieve relevant context dynamically.\n",
    "\n",
    "- **Use Case:**  \n",
    "  For chatbots, memory ensures that the bot understands and maintains context throughout a conversation.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Chains**  \n",
    "Chains are sequences of operations that transform inputs into outputs. LangChain allows developers to build complex workflows by chaining multiple components together.\n",
    "\n",
    "- **LLMChain:**  \n",
    "  The simplest type of chain, consisting of a prompt and an LLM.  \n",
    "  Example:  \n",
    "  ```python\n",
    "  from langchain.chains import LLMChain\n",
    "  from langchain.llms import OpenAI\n",
    "\n",
    "  llm = OpenAI(model=\"gpt-4\")\n",
    "  chain = LLMChain(llm=llm, prompt=prompt)\n",
    "  response = chain.run({\"context\": \"AI is transforming industries.\", \"question\": \"How is it used in healthcare?\"})\n",
    "  ```\n",
    "  \n",
    "- **Sequential Chains:**  \n",
    "  Combine multiple chains to perform more complex tasks, such as summarization followed by question-answering.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Tools and Agents**  \n",
    "Agents are decision-makers that dynamically decide which tools to use based on user input. Tools provide external capabilities, such as searching the web or accessing APIs.\n",
    "\n",
    "- **Tools:**  \n",
    "  Common tools include:\n",
    "  - **Web Search:** Retrieve real-time information.\n",
    "  - **Calculators:** Perform mathematical computations.\n",
    "  - **Databases:** Query structured or unstructured data.\n",
    "\n",
    "- **Agents:**  \n",
    "  Agents use prompts to decide which tool to invoke and how to handle responses.  \n",
    "  Example: An agent might search the web for information if a question cannot be answered using the LLM alone.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Data Connectors**  \n",
    "LangChain supports **Retrieval-Augmented Generation (RAG)** by integrating with external data sources. This makes LLMs more powerful and capable of providing accurate, context-specific answers.\n",
    "\n",
    "- **Data Sources:**  \n",
    "  - **Vector Databases:** Pinecone, Weaviate, FAISS.  \n",
    "  - **Document Loaders:** PDFs, Excel files, web scraping.  \n",
    "  - **APIs:** Integrate third-party APIs for live data retrieval.\n",
    "\n",
    "- **Embedding Models:**  \n",
    "  LangChain allows embeddings to be generated for indexing and searching data. This ensures relevant information is retrieved efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Evaluation**  \n",
    "LangChain includes tools for evaluating and debugging applications to ensure they meet performance requirements.\n",
    "\n",
    "- **Human-in-the-Loop (HITL):**  \n",
    "  Involve human evaluators to assess the quality of responses.  \n",
    "- **Automated Evaluation:**  \n",
    "  Use metrics like BLEU, ROUGE, or accuracy to measure performance.\n",
    "\n",
    "---\n",
    "\n",
    "**8. Deployment**  \n",
    "LangChain applications can be deployed on various platforms, making them scalable and production-ready.\n",
    "\n",
    "- **Cloud Platforms:** AWS, GCP, Azure.  \n",
    "- **Dockerization:** Containerize LangChain apps for easy deployment.  \n",
    "- **Integration with APIs:** Expose the functionality as RESTful APIs for external use.\n",
    "\n",
    "---\n",
    "\n",
    "**9. Advanced Features**  \n",
    "- **Streaming:** LangChain supports streaming responses for real-time applications like live chat interfaces.  \n",
    "- **Callbacks:** Monitor and log the internal workflow of chains and agents for debugging or tracking.\n",
    "\n",
    "---\n",
    "\n",
    "**Why These Components Matter**  \n",
    "Each component is modular and can be independently configured, allowing developers to:\n",
    "- Customize solutions for specific use cases.\n",
    "- Scale applications without overhauling existing structures.\n",
    "- Ensure high performance and efficiency by leveraging the best tools and integrations.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to focus on a specific component, or provide an example project that ties these components together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Defining the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the API key\n",
    "with open('../ignore/secret_key.json') as f:\n",
    "    os.environ['OPENAI_API_KEY'] = json.load(f)['secret_key']\n",
    "    \n",
    "\n",
    "# Defines the LLM\n",
    "# Creates an instance of a Large Language Model (LLM), specifically one provided by OpenAI\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature is a hyperparameter that influences the randomness of the responses generated by the model. A higher temperature value (usually ranging from 0 to 1) promotes more creative and varied responses. On the other hand, a lower temperature tends to cause the model to produce more deterministic and possibly more predictable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Sakura Garden Dining\" \n"
     ]
    }
   ],
   "source": [
    "# Send the prompt to LLM and capture the response\n",
    "nome = llm.invoke(\"I want to open a Japanese food restaurant. Suggest a fancy name for it.\")\n",
    "print(nome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, the string “I want to open a Japanese food restaurant. Suggest a fancy name for it.” serves as the prompt or input to the language model. It describes the task the user wants the model to perform: creatively generating a name for a new Japanese food restaurant. The model will use its natural language training and prior knowledge to generate a response that meets this request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates in the context of LangChain refer to structured ways of formatting input to large language models (LLMs) to improve their performance and adherence to desired behaviors.\n",
    "\n",
    "A prompt template defines a template sequence with placeholder variables that can be populated dynamically. This allows you to construct prompts in a consistent and programmatic manner, rather than hard-coding full prompts.\n",
    "\n",
    "Prompt templates in LangChain provide a structured and extensible way to interface with LLMs, making it easy to explore and optimize prompting strategies to improve language model performance on specific tasks or domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prompt template\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a {cuisine} restaurant. Suggest a fancy name for it.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code defines a PromptTemplate, a framework that allows you to create dynamic prompts for use with Large Language Models (LLMs). This approach is particularly useful when you want to generate custom prompts based on specific variables or when you want to reuse a prompt format with different data sets.\n",
    "\n",
    "**input_variables = ['cuisine']**: Defines a list of variables that can be used to populate the template. In this case, there is a single variable called 'cuisine'. This variable acts as a placeholder that will be replaced with a specific value when the template is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a Mexican restaurant. Suggest a fancy name for it.\n"
     ]
    }
   ],
   "source": [
    "# Use the previously defined template to generate a specific prompt,\n",
    "# inserting the value \"Italiana\" in place of the variable culinary\n",
    "p = prompt_template_name.format(cuisine = \"Mexican\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Sequences with LLMChain\n",
    "\n",
    "Chains in LangChain are sequences of operations that can process inputs and generate outputs by combining multiple components, including large language models (LLMs), other chains, and specialized tools or utilities.\n",
    "\n",
    "An LLMChain is a type of chain that allows you to interact with a large language model (LLM) in a structured way. It provides a simple interface for passing inputs to the LLM and retrieving its outputs.\n",
    "\n",
    "The LLMChain serves as a building block for many other constructs in LangChain, such as agents, tools, and more advanced chain types. By encapsulating the LLM interaction logic in a reusable and extensible component, LLMChain simplifies the process of building applications that leverage large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a Brazilian restaurant. Suggest a fancy name for it.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Brazilian',\n",
       " 'text': '\\n\\n\"Sabor do Brasil\" which means \"Taste of Brazil\" in Portuguese.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the chain and activate verbose\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, verbose = True)\n",
    "\n",
    "# Invoke the chain by passing a parameter to the prompt\n",
    "chain.invoke(\"Brazilian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code creates an instance of LLMChain, a class designed to chain or sequence operations using an LLM. This instance is configured to use a specific language model and a predefined prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a Thai restaurant. Suggest a fancy name for it.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Thai', 'text': '\\n\\n\"Silk & Spice Thai Bistro\"'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the chain and activate verbose\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, verbose = True)\n",
    "\n",
    "# Invoke the chain by passing a parameter to the prompt\n",
    "chain.invoke(\"Thai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
